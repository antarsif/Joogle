{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of IR Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Evaluation of a Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criteria\n",
    "\n",
    "#### Efficiency \n",
    "\n",
    "* How fast does it index?\n",
    "    * Large scale ... Number of docs/hour\n",
    "* How fast does it search?\n",
    "    * Latency as a function of index size\n",
    "* How good are the results?\n",
    "    * Filling information need in top $N$ documents\n",
    "\n",
    "#### Interface \n",
    "\n",
    "* Expressiveness of the query language\n",
    "    * Boolean vs simple queries\n",
    "    * Negation\n",
    "* Error tolerance\n",
    "* UI\n",
    "* Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Measure: User Happiness\n",
    "\n",
    "* Speed of response/size of index are factors\n",
    "* Blindingly fast, useless answers won't make a user happy\n",
    "* Need a way to quantify happiness\n",
    "\n",
    "#### Variation by Goal\n",
    "\n",
    "* Web engine\n",
    "    * User finds what s/he wants, returns for more searches\n",
    "    * Measure: returning user rate\n",
    "* eCommerce\n",
    "    * User finds product to buy\n",
    "    * Measure: searches that result in purchase\n",
    "* Enterprise (Gov, academic)\n",
    "    * Speed of finding information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing a Standard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Benchmark\n",
    "\n",
    "1. Establish information need\n",
    "2. Establish relevant documents\n",
    "3. Have multiple annotators perform the same evaluation\n",
    "\n",
    "#### Relevance \n",
    "\n",
    "* Most common proxy of user happiness: relevance\n",
    "* How to measure?\n",
    "    1. A benchmark document collection\n",
    "    2. A benchmark suite of queries\n",
    "    3. A binary (usually) classification of either relevant or nonrelevant for each $(q, d)$ pair\n",
    "* Assessed with respect to _information need_, not query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-annotator agreement\n",
    "\n",
    "* Kappa measure ($\\kappa$)\n",
    "    * Agreement between judgement for categorical evaluation\n",
    "    * Corrects for agreement by chance\n",
    "* $P(A)$: proportion of time judges agree\n",
    "* $P(E)$: What agreement would be by chance\n",
    "* Output: $0$ if chance, $1$ for total agreement\n",
    "\n",
    "$$\\kappa = \\frac{[P(A)-P(E)]}{[1-P(E)]}$$\n",
    "\n",
    "#### Example\n",
    "\n",
    "|  _           | Relevant | Not Relevant | Total |\n",
    "|--------------|----------|--------------|-------|\n",
    "| Relevant     |    300   |           20 | 320   |\n",
    "| Not Relevant |    10    |           70 | 80    |\n",
    "| **Total**    |**310**   |  **90**      | **400**|\n",
    "\n",
    "$$P(A) = \\frac{300+70}{400}=0.925$$\n",
    "\n",
    "$$P(J_1=Y \\cap J_2 = Y)=\\frac{320}{400}\\cdot\\frac{310}{400}=0.62$$\n",
    "\n",
    "$$P(J_1=N \\cap J_2 = N)=\\frac{90}{400}\\cdot\\frac{80}{400}=0.045$$\n",
    "\n",
    "$$P(E)=0.62+0.045=0.665$$\n",
    "\n",
    "$$\\kappa = \\frac{P(A)-P(E)}{1-P(E)}=\\frac{0.925-0.665}{1-0.665}=0.776$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\kappa > 0.8$ = good agreement\n",
    "* $0.67 < \\kappa < 0.8 \\rightarrow$ \"tentative conclusions\"\n",
    "* For $>2$ annotators: average pairwise kappas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available test collections\n",
    "\n",
    "* TREC conference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "Fraction of relevant retrieved docs.\n",
    "\n",
    "$$P(\\text{Relevant}|\\text{Retrieved}) = \\frac{TP}{TP+FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "Fraction of relevant docs retrieved.\n",
    "\n",
    "$$P(\\text{Retrieved | Relevant}) = \\frac{TP}{TP+FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "Total number of correct classifications. Commonly used in machine learning classification, but not necessarily as useful in IR.\n",
    "\n",
    "$$\\frac{TP+TN}{TP+FP+FN+TN} = \\frac{TP+TN}{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tradeoff\n",
    "\n",
    "* Can get high recall (low precision) by just retrieving all docs\n",
    "    * Retrieval is non-decreasing function of # docs retrieved\n",
    "* Ideally, precision is inversely proportional to # docs / recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined F Measure\n",
    "\n",
    "* Weighted harmonic mean ($F$) assesses precision/recall tradeoff\n",
    "* Balanced $F_1$\n",
    "    * Generally used\n",
    "    * $\\beta=1$ or $\\alpha=\\frac{1}{2}$\n",
    "* Conservative\n",
    "\n",
    "$$F=\\frac{1}{\\alpha\\frac{1}{P}+(1-\\alpha)\\frac{1}{R}} = \\frac{(\\beta^2+1)PR}{\\beta^2 P + R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranked Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision@R\n",
    "\n",
    "1. Set rank threshold $K$\n",
    "2. Compute % relevant in top $K$\n",
    "3. Ignore documents ranked lower than $K$\n",
    "\n",
    "#### Interpolated Average Precision\n",
    "\n",
    "* Early TREC competitions used $11$-point interpolated average precision\n",
    "* Take precision at 11 levels of recall varying from 0 to 1 by tenths of the documents\n",
    "* Using interpolation _(value for $0$ always interpolated)_\n",
    "* Average them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision (MAP)\n",
    "\n",
    "* Average of the precision value obtained for top $k$ documents, each time a relevant doc is retrieved\n",
    "* Avoids interpolation, fixed recall levels\n",
    "* Most common measure in papers\n",
    "* Good for web search?\n",
    "* Assumes user is interested in finding many relevant documents per query\n",
    "\n",
    "#### Intra/inter- system variance\n",
    "\n",
    "* Test collections generally do poorly on some information needs ($\\mathit{MAP}=0.1$) and well on others ($\\mathit{MAP}=0.7$)\n",
    "* Variance of _same system across queries_ is **much greater** than variance of _different systems on same query_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Consider rank position $K$ of first relevant doc\n",
    "* Reciprocal Rank score = $\\frac{1}{K}$\n",
    "* $\\mathit{MRR}$ is the mean RR across queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Issues\n",
    "\n",
    "### Absolute/Marginal- Relevance\n",
    "\n",
    "* A document can be redundant even if highly relevant\n",
    "* Same, redundant information from multiple sources\n",
    "\n",
    "### A/B Testing\n",
    "\n",
    "* Purpose: Test single change/innovation/idea\n",
    "* Have most users old system\n",
    "* Divert small subset of users (e.g., $1\\%$) to new system\n",
    "* Evaluate with automatic measure\n",
    "    * Clickthrough on first result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
