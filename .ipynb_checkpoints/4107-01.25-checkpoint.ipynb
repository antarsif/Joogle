{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Model & Phrase Indexing - January 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranked Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Boolean Queries\n",
    "\n",
    "#### Query Formulation Issues\n",
    "\n",
    "* Good for expert users who know their collections (e.g., librarians)\n",
    "    * Complex queries become difficult to modify\n",
    "* Not intuitive for most users\n",
    "* TBF: No other model quite replicates functionality of `NOT` operator\n",
    "\n",
    "#### Result Set Issues\n",
    "\n",
    "* Feast or famine\n",
    "    * Too specific $\\longrightarrow$ no results\n",
    "        * `AND`\n",
    "            * Augments precision, diminishes recall\n",
    "    * Too general $\\longrightarrow$ too many results\n",
    "        * `OR`\n",
    "            * Augments recall, diminishes precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Ranked Model\n",
    "\n",
    "#### Simpler Queries\n",
    "\n",
    "* Free-text\n",
    "    * No operators (no `AND`? `OR`?)\n",
    "    * Free word order\n",
    "    \n",
    "#### Ranked Results\n",
    "\n",
    "* Documents sorted by **relevance**\n",
    "    * Only show top $N$\n",
    "    * Doesn't overwhelm user\n",
    "* Score associated with each document\n",
    "    * Represents degree to which query/doc match\n",
    "    * How do we formulate this score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Word Queries\n",
    "\n",
    "#### How to Rank?\n",
    "\n",
    "**Strict Retrieval**\n",
    "* If it exists _\"as-is\"_ within document\n",
    "* What additional measures can we use?\n",
    "    * Normalized document frequency\n",
    "        * Normalized relative to length of document\n",
    "    * Document popularity\n",
    "        * Requires a community of users\n",
    "    * User preference\n",
    "        * Requires user data\n",
    "        \n",
    "**Tolerant Retrieval**\n",
    "* Combination of term distance and collection term frequency\n",
    "* For equally distant terms, \"strict\" methods can be used\n",
    "* <span style='background-color: lightyellow'>**Quiz:** 3 methods of tolerant retrieval</span> \n",
    "    * Soundex \n",
    "    * Edit distance\n",
    "    * $n$-gram overlap (bag of $n$-grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-word Queries\n",
    "\n",
    "#### How to Rank?\n",
    "\n",
    "* <u>A free-form query becomes a **Bag-of-Words**.</u>\n",
    "    * `\"rice importation canada\"` $\\longrightarrow$ `(canada, importation, rice)`\n",
    "    * `\"rice consompution usa\"` $\\longrightarrow$ `(consumption, rice, usa)`\n",
    "* Result: A measure between $[0,1]$.\n",
    "* Typical measure is the Jaccard Coefficient\n",
    "    * $J(A, A) = 1$\n",
    "    * $J(A, B) = \\begin{cases} 0 &\\textrm{if }A\\cap B=\\varnothing \\\\ |A\\cap B| / |A \\cup B| &\\textrm{otherwise} \\end{cases}$\n",
    "\n",
    "#### Example of Jaccard Coefficient\n",
    "<span style='background-color:lightyellow'>**Quiz:** Given $Q, D_1, D_2$, calculate $J(D_1, D_2)$</span>\n",
    "\n",
    "_(THIS IS ON THE QUIZ)_\n",
    "\n",
    "Let: \n",
    "* Query $Q=$`\"ides of march\"`.\n",
    "* Documents\n",
    "    \n",
    "    * $D_1=$`\"caesar died in march\"`\n",
    "\n",
    "    * $D_2=$`\"the long march\"`\n",
    "\n",
    "$$A \\cap B = \\{\\textrm{march}\\}$$\n",
    "$$A \\cup B = \\{\\textrm{caesar, died, in, march, the, long}\\}$$\n",
    "$$J(A,B) = |A \\cap B| / |A \\cup B| $$\n",
    "$$= \\frac{1}{6}$$\n",
    "\n",
    "#### Can we combine ideas?\n",
    "\n",
    "Term frequency with single word queries $+$ $J(A, B)$ for multi-word queries $\\longrightarrow \\: \\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Model of IR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal Definition\n",
    "\n",
    "* Let $t$ distinct, pre-processed terms be our index/vocabulary.\n",
    "* These _orthogonal_ terms form a _vector space_\n",
    "    * $\\dim V = t = |\\textrm{vocab}|$\n",
    "* Within a document/query $j$, each term $j_i$ is given an associated weight, $w_{ij} \\in \\mathbb{R}$.\n",
    "* Both documents and queries are exepressed as $t$-dimensional vectors\n",
    "    * $d_j=\\begin{bmatrix}w_{1j}, \\: w_{2j}, \\: \\ldots \\end{bmatrix}$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical Representation\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Take: \n",
    "* $D_1=2T_1 + 3T_2 + 5T_3$\n",
    "* $D_2=3T_1 + 7T_2 + T_3$\n",
    "* $Q=0T_1 + 0T_2 + 2T_3$\n",
    "* Which $D_i$ is more similar to $Q$?\n",
    "    * Cosine similarity\n",
    "\n",
    "**insert image from slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Collection \n",
    "\n",
    "* A collection of $n$ documents can be represented in the VSM by a term-document matrix\n",
    "* An entry in this matrix corresponds to **the weight of a term in the document**\n",
    "    * $0 \\rightarrow$ no significance/doesn't exist\n",
    "\n",
    "**finish matrix from slides**\n",
    "$$\\begin{bmatrix}& T_1 & T_2 & \\ldots & T_t \\\\ D_1\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency\n",
    "\n",
    "* More frequent terms in a document $\\implies$ more relevant to the document\n",
    "    * Notation: $f_{ij}=$ frequency of term $i$ in document $j$\n",
    "* Could normalize _term frequency (tf)_ by dividing by the frequency of the **most common term in the document**\n",
    "    * $tf_{ij} = \\frac{f_{ij}}{\\max_f(f_{ij} \\in j)}$\n",
    "\n",
    "#### Log-frequency Weighting\n",
    "\n",
    "Log-frequency weight of term $t \\in d$:\n",
    "\n",
    "$$w_{t,d} = \\begin{cases}1+\\log_{10}(\\mathit{tf}_{t,d}), & \\mathit{tf}_{t,j} > 0 \\\\ 0, &\\textrm{otherwise} \\end{cases}$$\n",
    "\n",
    "e.g., $0\\rightarrow0, 1\\rightarrow1, 2\\rightarrow1.3, 10\\rightarrow2, 1000\\rightarrow4,$ etc.     \n",
    "\n",
    "#### Simple Scoring with Term Frequencies Only\n",
    "\n",
    "* Score for a doc/query pair: sum $t$ in both $q, d$.\n",
    "    * Do we want to use log in uOttawa corpus? With such short documents, maybe $2 > 1$ is important\n",
    "* Score is $0$ if none of the terms are present in the doc\n",
    "$$\\sum_{t\\in q\\cap d} (1+\\log(\\mathit{tf}_{t,d}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Frequency\n",
    "\n",
    "#### Rare Terms\n",
    "\n",
    "* Rare terms are more informative than frequent terms\n",
    "    * i.e., stop words\n",
    "* Conider a term in the query that is **rare in the collection** (e.g., _arachnocentric_)\n",
    "* A document containing this term is very likely to be relevant to the query _arachnocentric_\n",
    "* $\\rightarrow$ We want a high weight for rare terms like this\n",
    "* What is the prior?\n",
    "    * What can we expect for doc frequencies?\n",
    "    * How is the requency of different words distributed within a collection?\n",
    "\n",
    "#### Text Properties\n",
    "\n",
    "* A few words are quite common\n",
    "    * 2 most frequent words (\"the\", \"of\") account for $~10\\%$ of word occurrences\n",
    "* Most words are very rare \n",
    "    * **Half the words in a corpus would only appear once**\n",
    "    * _hapax legomena_ (greek: \"read only once\")\n",
    "\n",
    "#### Long-tail distriution\n",
    "\n",
    "* Growth of vocabulary size\n",
    "* Perhaps advantageous to remove stopwords\n",
    "* Short postings for remaining words \n",
    "* Matching of \"rare\" words will lead to high relevance\n",
    "* Are additional entries really \"rare\" words - or _spelling errors_?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency\n",
    "\n",
    "* $\\mathit{df}_t$ is the <u>document</u> frequency of $t$\n",
    "    * $\\#$ docs containing $t$\n",
    "        * $|\\{d: d \\in D, t \\in d\\}|$ \n",
    "    * Inverse measure of informativeness \n",
    "    * $\\mathit{df}_t \\leq N$\n",
    "* Define inverse document frequency of $t$\n",
    "    * We take the $\\log$ of the fraction to \"dampen\" the effect of $\\mathit{idf}$\n",
    "\n",
    "$$ \\mathit{idf}_t = \\log_{10}(N/\\mathit{df_t}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>doc_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>calpurnia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>animal</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sunday</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fly</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>under</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        term  doc_freq\n",
       "0  calpurnia         1\n",
       "1     animal       100\n",
       "2     sunday      1000\n",
       "3        fly     10000\n",
       "4      under    100000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "terms = [\"calpurnia\", \"animal\", \"sunday\", \"fly\", \"under\", \"the\"]\n",
    "freqs = [1, 100, 1000, 10000, 100000, 1000000]\n",
    "\n",
    "docs = pd.DataFrame([*zip(terms, freqs)], columns=[\"term\", \"doc_freq\"])\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>calpurnia</td>\n",
       "      <td>1</td>\n",
       "      <td>0.778151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>animal</td>\n",
       "      <td>100</td>\n",
       "      <td>-1.221849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sunday</td>\n",
       "      <td>1000</td>\n",
       "      <td>-2.221849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fly</td>\n",
       "      <td>10000</td>\n",
       "      <td>-3.221849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>under</td>\n",
       "      <td>100000</td>\n",
       "      <td>-4.221849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        term  doc_freq       idf\n",
       "0  calpurnia         1  0.778151\n",
       "1     animal       100 -1.221849\n",
       "2     sunday      1000 -2.221849\n",
       "3        fly     10000 -3.221849\n",
       "4      under    100000 -4.221849"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log10\n",
    "\n",
    "def idf(df_t):\n",
    "    return log10(len(docs)/df_t)\n",
    "\n",
    "docs[\"idf\"] = docs.doc_freq.apply(idf)\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Frequency vs. Collection Frequency\n",
    "\n",
    "* Collection frequency of $t$ is $\\#$ of occurrences of $t$ in the _collection_, counting multiple occurrences\n",
    "\n",
    "#### Characterizing our Collection\n",
    "\n",
    "* Stopwords versus doc frequencies\n",
    "    * What happens in a small collection?\n",
    "        * Not many words remaining\n",
    "    * Specialized collection?\n",
    "        * Might not make a big impact, might make huge impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Weighting\n",
    "\n",
    "* The $\\mathit{tf-idf}$ weight is the product $\\mathit{tf}_{t,d} \\times \\mathit{idf}_t$\n",
    "* **Best known weighting scheme in IR**\n",
    "* Increases with $\\#$ occurrences $\\in$ document, rarity of term in collection\n",
    "\n",
    "\n",
    "$$ w_{t,d} = \\log(1+\\mathit{tf}_{t,d}) \\times \\log_{10}(N/\\mathit{df}_t) $$\n",
    "\n",
    "#### Example\n",
    "\n",
    "```python\n",
    "D1 = \"Analysis of various operating systems and impact on programming. Principles of operating systems and design issues.\"\n",
    "D2 = \"Computer graphics for various systems. Graphical design. Application of graphics in 2D.\"\n",
    "D3 = \"Programming paradigms. Usability and impact for applicative systems.\"\n",
    "\n",
    "query = \"design paradigm shift\"\n",
    "```\n",
    "\n",
    "<span style='background-color: lightyellow'>**Quiz: This was skipped in class - do at home as exercise** (choose a few words) (THIS IS ON IT)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching/Ranking Algorithm\n",
    "\n",
    "1. Convert all documents $\\in$ collection $D$ to $\\mathit{tf-idf}$ weighted vectors, $d_{j'}$, using vocabulary $V$.\n",
    "2. Convert the query to a weighted vector $q$.\n",
    "3. For each $d_{j'} \\in D$: compute score $s_j = \\mathit{sim}(d_{j'}, q)$\n",
    "    * **Matrix multiply $q \\times D'$ instead of for loop**\n",
    "4. Sort documents by decreasing scores\n",
    "5. Present top scores to users\n",
    "\n",
    "#### Query Vector\n",
    "\n",
    "* Deal with query as document and apply $\\mathit{tf-idf}$\n",
    "* Or just use $\\mathit{tf}$\n",
    "* Ask user to provide weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Measures\n",
    "\n",
    "#### Inner Product\n",
    "\n",
    "* Similarity between vectors for $d_i$ and $q$ can be computed as the dot product\n",
    "    * It then follows that we can use matrix multiplications rather than for loop\n",
    "* For binary vectors, inner product is the number of matches query terms in the document (size of intersection)\n",
    "    * This is also dot product! \n",
    "    * Literally always just dot product\n",
    "\n",
    "#### Cosine\n",
    "\n",
    "* Cosine similarity measures cosine of the angle between two vectors\n",
    "* Inner product normalized by the vector lengths\n",
    "\n",
    "**add rest of slide**\n",
    "\n",
    "#### Length normalization\n",
    "\n",
    "* A vector can be length-normalized by dividing each of its components by its length; for this we use the $L_2$ norm\n",
    "$$||\\vec{x}||_2 = \\sqrt{\\sum_i(x^2_i)}$$\n",
    "\n",
    "#### Cosine for $L_2$ normalized vectors\n",
    "\n",
    "$$\\cos(\\vec{q}, \\vec{d}) = \\vec{q}\\cdot\\vec{d}=\\sum^{|V|}_{i=1}q_id_i$$\n",
    "\n",
    "Equivalent to dot-product if $q, d$ normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VSM Recap\n",
    "\n",
    "* Simple, mathematically based approach\n",
    "* Considers local ($\\mathit{tf}$) and global ($\\mathit{idf}$) word occurrence frequencies\n",
    "* Partial matching, ranked results\n",
    "* Works well in practice despite obvious weaknesses\n",
    "* Efficient implementations for large collections\n",
    "\n",
    "### VSM Issues\n",
    "\n",
    "* Lacks control of the boolean model\n",
    "    * Cannot require a term to appear\n",
    "    * Given two term query `A B`, scoring may prefer a document with a single term having a large weight than documents with both terms\n",
    "* Lacks syntactic information (phrases, word order, proximity)\n",
    "* Lacks semantic information/word sense (as does boolean model)\n",
    "* Assumes independence between terms\n",
    "    * There ~~might be~~ **IS** correlation between terms\n",
    "        * wheeeeee embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Phrases\n",
    "\n",
    "What do you think of the BOW's independence assumption?\n",
    "\n",
    "`\"electric car charging Ottawa\"` $\\rightarrow$ `(car, charging, electric, Ottawa)`\n",
    "\n",
    "* Loses the order\n",
    "   * `\"electric car\"` should be kept in order\n",
    "* Correlation between terms\n",
    "    * `(electric, charging)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrases Found in External Resources\n",
    "\n",
    "Using external resource like Oxford dictionary of computer science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary Building\n",
    "\n",
    "* Rather than processing individual tokens, pre-process documents to identify _phrases_\n",
    "    * Re-write with underscores?\n",
    "* Include phrases in dictionary\n",
    "* Index the document collection with the dictionary\n",
    "* Will _significantly_ augment size of the dictionary\n",
    "\n",
    "### Automatic Extraction of Phrases\n",
    "\n",
    "Recall the Jaccard Coefficient $J(A, B)$. Apply to all bi-grams.\n",
    "\n",
    "* $A \\cap B$: `[bi for bi in bigrams if bi == (A, B)]`\n",
    "* $A \\cup B$: `[bi for bi in bigrams if (A in bi) or (B in bi)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('analysis', 'of'),\n",
       " ('of', 'various'),\n",
       " ('various', 'operating'),\n",
       " ('operating', 'systems'),\n",
       " ('systems', 'and')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D1 = \"Analysis of various operating systems and impact on programming. Principles of operating systems and design issues.\"\n",
    "D2 = \"Computer graphics for various systems. Graphical design. Application of graphics in 2D.\"\n",
    "D3 = \"Programming paradigms. Usability and impact for applicative systems.\"\n",
    "\n",
    "# https://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python\n",
    "text = map(lambda x: x.lower().replace(\".\", \"\"), [D1, D2, D3])\n",
    "bigrams = [b for l in text for b in zip(l.lower().split(\" \")[:-1], l.lower().split(\" \")[1:])]\n",
    "\n",
    "print(len(bigrams))\n",
    "bigrams[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/9=\n",
      "0.2222222222222222\n",
      "1/5=\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "def J(A, B):\n",
    "    num = [bi for bi in bigrams if bi == (A, B)]\n",
    "    denom = [bi for bi in bigrams if (A in bi) or (B in bi)]\n",
    "    \n",
    "    print(f\"{len(num)}/{len(denom)}=\")\n",
    "    \n",
    "    return 0 if len(denom) == 0 else len(num) / len(denom)\n",
    "\n",
    "print(J(\"operating\", \"systems\"))\n",
    "print(J(\"graphical\", \"design\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Jaccard is only one of many possible measures\n",
    "* Needs a threshold on coefficient to decide (yes/no) on inclusion in dictionary\n",
    "    * How could we learn this?\n",
    "    \n",
    "### Query Formats\n",
    "\n",
    "#### Explicit Phrases\n",
    "\n",
    "* If phrases are included in the dictionary, we should find them in the dictionary\n",
    "* Provide a syntax to identify phrases\n",
    "    * e.g., use of quotes\n",
    "        * \"operating system\" class\n",
    "        * \"electric car\" \"car charging station\" Ottawa\n",
    "        \n",
    "#### Implicit Phrases\n",
    "\n",
    "\"stanford university palo alto\" $\\rightarrow$ \"stanford university\" \"palo alto\", stanford \"university palo\" alto, etc.\n",
    "\n",
    "### Retrieval Strategy\n",
    "\n",
    "* User identified query phrases\n",
    "    * What if phrase query returns nothing?\n",
    "    * Two-step queries\n",
    "        * Phrase\n",
    "        * BOW\n",
    "* Implicit query phrases\n",
    "    * Iteratively go thru most specific to generic\n",
    "    * How to combine the ranking when multiple equally specific queries provide different results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models (Context-Sensitive Correction)\n",
    "\n",
    "this was skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Modules / Summary / Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz\n",
    "\n",
    "* All material from first 3 lectures\n",
    "* BRING A CALCULATOR\n",
    "* **Definitions**\n",
    "    * _In IR, the frequent words \"a\", \"in\", \"the\" are called <u>stopwords</u>._\n",
    "* Short answers\n",
    "    * Fewer of these\n",
    "    * _Explain the information need behind this query: \"olympic game 2020\"_\n",
    "        * Location\n",
    "        * Time\n",
    "* True/False\n",
    "    * _Boolean queries cannot provide ranking of documents. T_\n",
    "* Algorithm demonstration\n",
    "    * Given 3 documents ...\n",
    "    * _Assuming term frequency normalization, find $\\mathit{tf}$ of \"drives\" in $D_3$._\n",
    "    * _What is the $mathit{idf}$ of the word \"car\" in the collection?_\n",
    "    * _Assuming a query \"nice old table\" and a vector space model for retrieval, what would the ranking of the documents be and why? Show the term-document matrix with the $\\mathit{tf-idf}$ weight of each query word for each document. Assume cosine similarity for scoring, all words have weight of 1 in query._\n",
    "    * _If a boolean retrieval model was used, which document(s) would be retrieved with the following query: \"car AND (old OR broken)\"_\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
